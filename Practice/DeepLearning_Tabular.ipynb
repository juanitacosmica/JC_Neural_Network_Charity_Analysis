{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5ff786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import our input dataset\n",
    "attrition_df = pd.read_csv('Resources/HR-Employee-Attrition.csv')\n",
    "attrition_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45196b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate our categorical variable list\n",
    "attrition_cat = attrition_df.dtypes[attrition_df.dtypes == \"object\"].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d6500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of unique values in each column\n",
    "attrition_df[attrition_cat].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4a97c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a OneHotEncoder instance\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit and transform the OneHotEncoder using the categorical variable list\n",
    "encode_df = pd.DataFrame(enc.fit_transform(attrition_df[attrition_cat]))\n",
    "\n",
    "# Add the encoded variable names to the DataFrame\n",
    "encode_df.columns = enc.get_feature_names(attrition_cat)\n",
    "encode_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9f2253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge one-hot encoded features and drop the originals\n",
    "attrition_df = attrition_df.merge(encode_df,left_index=True, right_index=True)\n",
    "attrition_df = attrition_df.drop(attrition_cat,1)\n",
    "attrition_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d00973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our preprocessed data into our features and target arrays\n",
    "y = attrition_df[\"Attrition_Yes\"].values\n",
    "X = attrition_df.drop([\"Attrition_Yes\",\"Attrition_No\"],1).values\n",
    "\n",
    "# Split the preprocessed data into a training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17217c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abeed0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model - deep neural net\n",
    "number_input_features = len(X_train[0])\n",
    "hidden_nodes_layer1 = 8\n",
    "hidden_nodes_layer2 = 5\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
    ")\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da21336a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deada3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "fit_model = nn.fit(X_train,y_train,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a458283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af3a7de",
   "metadata": {},
   "source": [
    "### Generating Checkpoints\n",
    "\n",
    "1. Import dependencies.\n",
    "2. Import the input dataset.\n",
    "3. Generate categorical variable list.\n",
    "4. Create a OneHotEncoder instance.\n",
    "5. Fit and transform the OneHotEncoder.\n",
    "6. Add the encoded variable names to the DataFrame.\n",
    "7. Merge one-hot encoded features and drop the originals.\n",
    "8. Split the preprocessed data into features and target arrays.\n",
    "9. Split the preprocessed data into training and testing dataset.\n",
    "10. Create a StandardScaler instance.\n",
    "11. Fit the StandardScaler.\n",
    "12. Scale the data.\n",
    "13. Define the model.\n",
    "14. Add first and second hidden layers.\n",
    "15. Add the output layer.\n",
    "16. Check the structure of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43abf0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import checkpoint dependencies\n",
    "import os\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define the checkpoint path and filenames\n",
    "os.makedirs(\"checkpoints/\",exist_ok=True)\n",
    "checkpoint_path = \"checkpoints/weights.{epoch:02d}.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6135c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Create a callback that saves the model's weights every epoch\n",
    "cp_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    "    save_freq='epoch')\n",
    "\n",
    "# Train the model\n",
    "fit_model = nn.fit(X_train_scaled,y_train,epochs=100,callbacks=[cp_callback])\n",
    "\n",
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b488842b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model - deep neural net\n",
    "number_input_features = len(X_train_scaled[0])\n",
    "hidden_nodes_layer1 = 8\n",
    "hidden_nodes_layer2 = 5\n",
    "\n",
    "nn_new = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn_new.add(\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
    ")\n",
    "\n",
    "# Second hidden layer\n",
    "nn_new.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "# Output layer\n",
    "nn_new.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the model\n",
    "nn_new.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Restore the model weights\n",
    "nn_new.load_weights(\"checkpoints/weights.100.hdf5\")\n",
    "\n",
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn_new.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee236214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export our model to HDF5 file\n",
    "nn_new.save(\"trained_attrition.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9dbaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model to a new object\n",
    "nn_imported = tf.keras.models.load_model('trained_attrition.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e23b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the completed model using the test data\n",
    "model_loss, model_accuracy = nn_imported.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
